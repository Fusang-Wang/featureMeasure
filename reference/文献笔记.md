## Implicit semantic data augmentation

### PROS

* 对于语义隐空间进行讨论，有效的做了数据增强；提出了隐式数据增强方法，提高了数据增强方法在supervised learning 的效率。

### CONS

* supervised learning or semi-supervised learning，没有涉及 generative model
* lack of metrics 缺少量化评价指标，增强效果是通过分类准确率的提升而获得的，对于是否增强并不直观
* 对前提假设：deep feature space的线性仅就经验讨论缺乏数学证明——我们通过损失函数确保feature space的线性。核心问题是：如何让网络学习到合理的数据结构。

## Wasserstein adversarial regularization 

### PROS

* solution of the **sharp entropic variant** of the optimal transport problem
* <img src="C:\Users\fusangwang\AppData\Roaming\Typora\typora-user-images\image-20210720214143087.png" alt="image-20210720214143087" style="zoom:60%;" />
* Differential properties of **sinkhorn approximation** for learning with wasserstein distance

### CONS

* 

## Rectify Heterogeneous Models with Semantic Mapping



## AE-OT-GAN

### Cell decomposition

* similar images share the same activation path, then the neural naetwork becomes a linear function,wich explain the linearilty of the deep feature space
* For generative model, leaky relu function keeps the piecewise linearilty of the activation function, and for upsampling, similar feature maps results in la linear transform. 
* to certain extend, latent code interpolation --> linear transform of the images: the tiny variation of the latent code hasn't change greatly the activation map of the generative model, results in linear enhencement of certain feature. positive activated feature ->0-> negative activated feature
* then how to explain the immersion of new feature? the sampled latent code actives a new activation path -> diifferent        

